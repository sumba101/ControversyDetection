{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(tathagataraha/haha-cls1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert code here.\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm, neighbors\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, mean_squared_error, f1_score, accuracy_score\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import pytorch_lightning as pl\n",
    "import neptune\n",
    "import string\n",
    "import pickle\n",
    "import sys\n",
    "with open(\"token\", \"r\") as f:\n",
    "    token = f.read()\n",
    "neptune.init(project_qualified_name='tathagataraha/haha-cls1',\n",
    "                 api_token=token,\n",
    "             )\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "# sent_encoder = SentenceTransformer('bert-base-nli-mean-tokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 4 GPU(s) available.\n",
      "We will use the GPU: GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda:0\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['bert-base-uncased', 'roberta-base', 'google/electra-base-discriminator', 'xlnet-base-cased', '/scratch/tr/bert-base-uncased-tapt','/scratch/tr/distilgpt2-tapt', '/scratch/tr/roberta-base-tapt']\n",
    "labels = ['is_humor', 'humor_rating', 'humor_controversy', 'offense_rating']\n",
    "model_num = 3\n",
    "label_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'model' : models[model_num],\n",
    "    'label' : labels[label_num],\n",
    "    'valid_size' : 0.2,\n",
    "    'rnd' : 42,\n",
    "    'max_len' : 64,\n",
    "    'train_batch' : 32,\n",
    "    'valid_batch' : 32,\n",
    "    'epochs' : 10,\n",
    "    'lr' : 1e-05,\n",
    "    'dropout' : 0.1,\n",
    "    'file': ''.join(random.choices(string.ascii_lowercase + string.digits, k = 20)),\n",
    "    'lexical' : 1,\n",
    "    'hurtlex' : 1,\n",
    "    'gpu' : 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'xlnet-base-cased',\n",
       " 'label': 'is_humor',\n",
       " 'valid_size': 0.2,\n",
       " 'rnd': 42,\n",
       " 'max_len': 64,\n",
       " 'train_batch': 32,\n",
       " 'valid_batch': 32,\n",
       " 'epochs': 10,\n",
       " 'lr': 1e-05,\n",
       " 'dropout': 0.1,\n",
       " 'file': 'rbh6894bsn7n8c7kf5p2',\n",
       " 'lexical': 1,\n",
       " 'hurtlex': 1,\n",
       " 'gpu': 0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pl.seed_everything(params['rnd'])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(params['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "valid = pd.read_csv('data/dev.csv')\n",
    "train = pd.concat([train, valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainvec = pd.read_csv('hurtlex_features/trainvec.csv')\n",
    "trainvec = trainvec.values.tolist()\n",
    "devvec = pd.read_csv('hurtlex_features/devvec.csv')\n",
    "devvec = devvec.values.tolist()\n",
    "trainvec.extend(devvec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(trainvec)):\n",
    "    trainvec[i][1] = int(trainvec[i][1][1])\n",
    "    trainvec[i][-1] = int(trainvec[i][-1][1])\n",
    "    trainvec[i] = trainvec[i][1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000\n"
     ]
    }
   ],
   "source": [
    "with open('lexical_features/train.pickle', 'rb') as f:\n",
    "    lx = pickle.load(f)\n",
    "with open('lexical_features/dev.pickle', 'rb') as f:\n",
    "    lx2 = pickle.load(f)\n",
    "lx.extend(lx2)\n",
    "print(len(lx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['lx'] = len(lx[0])\n",
    "params['hurt'] = len(trainvec[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['hurtlex'] = trainvec\n",
    "train['lexical'] = lx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>hurtlex</th>\n",
       "      <th>lexical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[114, 10, 22, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[189, 9, 6, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[112, 3, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[139, 4, 6, 4, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[72, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>8996</td>\n",
       "      <td>boss: what are you doing inventor of the bagpi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[73, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>8997</td>\n",
       "      <td>I told him his views were pretty extreme and i...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[149, 2, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8998</td>\n",
       "      <td>\"Mum, all the black kids call each other Nigga...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[132, 10, 5, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8999</td>\n",
       "      <td>In honor of Fathers Day, I'm gonna bring you \"...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[240, 14, 22, 6, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9000</td>\n",
       "      <td>I don't know why Coca-Cola and Pepsi are fight...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[134, 4, 5, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  is_humor  \\\n",
       "0       1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1       2  A man inserted an advertisement in the classif...         1   \n",
       "2       3  How many men does it take to open a can of bee...         1   \n",
       "3       4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4       5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "..    ...                                                ...       ...   \n",
       "995  8996  boss: what are you doing inventor of the bagpi...         1   \n",
       "996  8997  I told him his views were pretty extreme and i...         0   \n",
       "997  8998  \"Mum, all the black kids call each other Nigga...         1   \n",
       "998  8999  In honor of Fathers Day, I'm gonna bring you \"...         0   \n",
       "999  9000  I don't know why Coca-Cola and Pepsi are fight...         1   \n",
       "\n",
       "     humor_rating  humor_controversy  offense_rating  \\\n",
       "0            2.42                1.0            0.20   \n",
       "1            2.50                1.0            1.10   \n",
       "2            1.95                0.0            2.40   \n",
       "3            2.11                1.0            0.00   \n",
       "4            2.78                0.0            0.10   \n",
       "..            ...                ...             ...   \n",
       "995          2.06                1.0            0.25   \n",
       "996           NaN                NaN            0.10   \n",
       "997          1.94                0.0            2.95   \n",
       "998           NaN                NaN            0.00   \n",
       "999          2.63                0.0            0.30   \n",
       "\n",
       "                                               hurtlex  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "..                                                 ...   \n",
       "995  [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "999  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               lexical  \n",
       "0    [114, 10, 22, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,...  \n",
       "1    [189, 9, 6, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2    [112, 3, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "3    [139, 4, 6, 4, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4    [72, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "..                                                 ...  \n",
       "995  [73, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "996  [149, 2, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "997  [132, 10, 5, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, ...  \n",
       "998  [240, 14, 22, 6, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,...  \n",
       "999  [134, 4, 5, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4...  \n",
       "\n",
       "[9000 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train.text, train[params['label']], test_size=params['valid_size'], random_state=params['rnd'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    try:\n",
    "        return len(text.split())\n",
    "    except:\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20.712666666666667, 63, 0, 9000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total = 0\n",
    "maxw = 0\n",
    "large_count = 0\n",
    "for i in train.text:\n",
    "    temp = count_words(i)\n",
    "    total += temp\n",
    "    maxw = temp if temp > maxw else maxw\n",
    "    large_count += 1 if temp > 64 else 0\n",
    "total/len(train.text), maxw, large_count, len(train.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "      <th>hurtlex</th>\n",
       "      <th>lexical</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>TENNESSEE: We're the best state. Nobody even c...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.42</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[114, 10, 22, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>A man inserted an advertisement in the classif...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.10</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[189, 9, 6, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>How many men does it take to open a can of bee...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.40</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[112, 3, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Told my mom I hit 1200 Twitter followers. She ...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.11</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[139, 4, 6, 4, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Roses are dead. Love is fake. Weddings are bas...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.78</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[72, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>8996</td>\n",
       "      <td>boss: what are you doing inventor of the bagpi...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.06</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>[0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[73, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>8997</td>\n",
       "      <td>I told him his views were pretty extreme and i...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[149, 2, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>8998</td>\n",
       "      <td>\"Mum, all the black kids call each other Nigga...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.94</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.95</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[132, 10, 5, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>8999</td>\n",
       "      <td>In honor of Fathers Day, I'm gonna bring you \"...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[240, 14, 22, 6, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>9000</td>\n",
       "      <td>I don't know why Coca-Cola and Pepsi are fight...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.63</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.30</td>\n",
       "      <td>[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[134, 4, 5, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       id                                               text  is_humor  \\\n",
       "0       1  TENNESSEE: We're the best state. Nobody even c...         1   \n",
       "1       2  A man inserted an advertisement in the classif...         1   \n",
       "2       3  How many men does it take to open a can of bee...         1   \n",
       "3       4  Told my mom I hit 1200 Twitter followers. She ...         1   \n",
       "4       5  Roses are dead. Love is fake. Weddings are bas...         1   \n",
       "..    ...                                                ...       ...   \n",
       "995  8996  boss: what are you doing inventor of the bagpi...         1   \n",
       "996  8997  I told him his views were pretty extreme and i...         0   \n",
       "997  8998  \"Mum, all the black kids call each other Nigga...         1   \n",
       "998  8999  In honor of Fathers Day, I'm gonna bring you \"...         0   \n",
       "999  9000  I don't know why Coca-Cola and Pepsi are fight...         1   \n",
       "\n",
       "     humor_rating  humor_controversy  offense_rating  \\\n",
       "0            2.42                1.0            0.20   \n",
       "1            2.50                1.0            1.10   \n",
       "2            1.95                0.0            2.40   \n",
       "3            2.11                1.0            0.00   \n",
       "4            2.78                0.0            0.10   \n",
       "..            ...                ...             ...   \n",
       "995          2.06                1.0            0.25   \n",
       "996           NaN                NaN            0.10   \n",
       "997          1.94                0.0            2.95   \n",
       "998           NaN                NaN            0.00   \n",
       "999          2.63                0.0            0.30   \n",
       "\n",
       "                                               hurtlex  \\\n",
       "0    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2    [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "3    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4    [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "..                                                 ...   \n",
       "995  [0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "996  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "997  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "998  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "999  [0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                               lexical  \n",
       "0    [114, 10, 22, 0, 0, 1, 0, 0, 2, 0, 0, 0, 0, 0,...  \n",
       "1    [189, 9, 6, 4, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "2    [112, 3, 3, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "3    [139, 4, 6, 4, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "4    [72, 3, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "..                                                 ...  \n",
       "995  [73, 2, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]  \n",
       "996  [149, 2, 1, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0...  \n",
       "997  [132, 10, 5, 0, 1, 1, 1, 1, 2, 0, 0, 0, 0, 0, ...  \n",
       "998  [240, 14, 22, 6, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0,...  \n",
       "999  [134, 4, 5, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4...  \n",
       "\n",
       "[9000 rows x 8 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLabelDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataframe, tokenizer, max_len, t = False):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.text = dataframe.text\n",
    "        self.hurtlex = dataframe.hurtlex\n",
    "        self.lexical = dataframe.lexical\n",
    "#         self.emoji = dataframe.emoji\n",
    "#         self.hash = dataframe.segmented_hash\n",
    "        self.t = t\n",
    "        if not self.t:\n",
    "            self.targets = self.data[params['label']]\n",
    "        self.max_len = max_len\n",
    "#         print(self.targets)\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text = str(self.text[index])\n",
    "        text = \" \".join(text.split())\n",
    "        hurt = np.asarray(self.hurtlex[index])\n",
    "        lx = np.asarray(self.lexical[index])\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "        token_type_ids = inputs[\"token_type_ids\"]\n",
    "#         h_text = self.hash[index]\n",
    "#         h_text = \" \".join(h_text)\n",
    "#         inputs = self.tokenizer.encode_plus(\n",
    "#             h_text,\n",
    "#             None,\n",
    "#             truncation=True,\n",
    "#             add_special_tokens=True,\n",
    "#             max_length=self.max_len,\n",
    "#             pad_to_max_length=True,\n",
    "#             return_attention_mask = True,\n",
    "#             return_token_type_ids=True\n",
    "#         )\n",
    "#         h_ids = inputs['input_ids']\n",
    "#         h_mask = inputs['attention_mask']\n",
    "#         h_token_type_ids = inputs[\"token_type_ids\"]\n",
    "#         h_inputs\n",
    "#         emoji = getEmojiEmbeddings(self.emoji[index])\n",
    "        if self.t:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'hurtlex' : torch.tensor(hurt, dtype=torch.long),\n",
    "                'lexical' : torch.tensor(lx, dtype=torch.long)\n",
    "#                 'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "#                 'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "#                 'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "#                 'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'ids': torch.tensor(ids, dtype=torch.long),\n",
    "                'mask': torch.tensor(mask, dtype=torch.long),\n",
    "                'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n",
    "                'hurtlex' : torch.tensor(hurt, dtype=torch.long),\n",
    "                'lexical' : torch.tensor(lx, dtype=torch.long),\n",
    "#                 'h_ids': torch.tensor(h_ids, dtype=torch.long),\n",
    "#                 'h_mask': torch.tensor(h_mask, dtype=torch.long),\n",
    "#                 'h_token_type_ids': torch.tensor(h_token_type_ids, dtype=torch.long),\n",
    "#                 'emoji' : torch.tensor(emoji, dtype=torch.long),\n",
    "                'targets': torch.tensor(self.targets[index], dtype=torch.long)\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (9000, 8)\n",
      "TRAIN Dataset: (7200, 8)\n",
      "TEST Dataset: (1800, 8)\n"
     ]
    }
   ],
   "source": [
    "# Creating the dataset and dataloader for the neural network\n",
    "# train_size = 0.85\n",
    "# train_data=train.sample(frac=1 - params['valid_size'],random_state=params['rnd'])\n",
    "# test_data=train.drop(train_data.index).reset_index(drop=True)\n",
    "# train_data = train_data.reset_index(drop=True)\n",
    "train_data, test_data = train_test_split(train, test_size=params['valid_size'],random_state=params['rnd'])\n",
    "\n",
    "test_data=test_data.reset_index(drop=True)\n",
    "train_data = train_data.reset_index(drop=True)\n",
    "valid_data = test_data\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(train.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_data.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_data.shape))\n",
    "\n",
    "training_set = MultiLabelDataset(train_data, tokenizer, params['max_len'])\n",
    "testing_set = MultiLabelDataset(test_data, tokenizer, params['max_len'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data.to_csv('data/task_1_val.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_params = {'batch_size': params['train_batch'],\n",
    "                'shuffle': True,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "test_params = {'batch_size': params['valid_batch'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_params)\n",
    "testing_loader = DataLoader(testing_set, **test_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModelClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.l1 = AutoModel.from_pretrained(params['model'])\n",
    "        self.pre_classifier_1 = torch.nn.Linear(768, 768)\n",
    "        self.final_layer_dim = 768\n",
    "        if params['lexical']:\n",
    "            self.final_layer_dim += params['lx']\n",
    "            self.hidden_lex = torch.nn.Linear(params['lx'], params['lx'])\n",
    "        if params['hurtlex']:\n",
    "            self.final_layer_dim += params['hurt']\n",
    "            self.hidden_hurt = torch.nn.Linear(params['hurt'], params['hurt'])\n",
    "        if params['lexical'] or params['hurtlex']:\n",
    "            self.pre_classifier_2 = torch.nn.Linear(self.final_layer_dim, self.final_layer_dim)\n",
    "#         print(self.pre_classifier_2)\n",
    "        self.classifier = torch.nn.Linear(self.final_layer_dim, 2)\n",
    "        self.dropout = torch.nn.Dropout(params['dropout'])\n",
    "        self.total_loss = 0\n",
    "        self.val_loss = 0\n",
    "        self.val_batch = 0\n",
    "        self.batch_count = 0\n",
    "        self.epoch = 0\n",
    "        self.macrof1 = 0\n",
    "        self.acc = 0\n",
    "        self.f1 = 0\n",
    "        \n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "        self.test_preds = []\n",
    "\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, lx, hurt):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state_1 = output_1[0]\n",
    "        pooler_1 = hidden_state_1[:, 0]\n",
    "        pooler_1 = self.pre_classifier_1(pooler_1)\n",
    "        pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "        pooler_1 = self.dropout(pooler_1)\n",
    "#         print(pooler_1.shape)\n",
    "        if params['lexical']:\n",
    "            pooler_2 = self.hidden_lex(lx)\n",
    "            pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "            pooler_2 = self.dropout(pooler_2)\n",
    "#             print(pooler_2.shape)\n",
    "            pooler_1 = torch.cat((pooler_1, pooler_2), 1)\n",
    "        \n",
    "        if params['hurtlex']:\n",
    "            pooler_2 = self.hidden_hurt(hurt)\n",
    "            pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "            pooler_2 = self.dropout(pooler_2)\n",
    "#             print(pooler_2.shape)\n",
    "            pooler_1 = torch.cat((pooler_1, pooler_2), 1)\n",
    "#         print(pooler_1.shape)\n",
    "        if params['lexical'] or params['hurtlex']:\n",
    "#             print(pooler_1.shape)\n",
    "            pooler_1 = self.pre_classifier_2(pooler_1)\n",
    "            pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "            pooler_1 = self.dropout(pooler_1)\n",
    "        output = self.classifier(pooler_1)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params =  self.parameters(), lr=params['lr'])\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        targets = batch['targets']\n",
    "        lx = batch['lexical']\n",
    "        hurt = batch['hurtlex']\n",
    "        lx = lx.to(torch.float32)\n",
    "        hurt = hurt.to(torch.float32)\n",
    "        outputs = self.forward(ids, mask, token_type_ids, lx, hurt)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "        self.total_loss += loss\n",
    "        self.batch_count += 1\n",
    "        logger_logs = {'training_loss': loss}\n",
    "        logger_logs = {'losses': logger_logs} # optional (MUST ALL BE TENSORS)\n",
    "        output = {\n",
    "            'loss': loss, # required\n",
    "            'progress_bar': {'training_loss': loss}, # optional (MUST ALL BE TENSORS)\n",
    "            'log': logger_logs\n",
    "        }\n",
    "        # return a dict\n",
    "        return output\n",
    "    def on_epoch_end(self):\n",
    "        self.epoch += 1\n",
    "        params['epoch'] = self.epoch\n",
    "#         params['file'] = ''.join(random.choices(string.ascii_lowercase + string.digits, k = 20))\n",
    "#         trainer.checkpoint_callback.format_checkpoint_name(0, 0, metrics=dict(file=params['file']))\n",
    "#         trainer.checkpoint_callback.on_epoch_end()\n",
    "        print(f'Epoch: {self.epoch}, Loss:  {self.total_loss/self.batch_count}')\n",
    "        neptune.create_experiment(params = params)\n",
    "        neptune.log_metric('train_loss', self.total_loss/self.batch_count)\n",
    "        neptune.log_metric('val_loss', self.val_loss/self.val_batch)\n",
    "        neptune.log_metric('acc', self.acc)\n",
    "        neptune.log_metric('f1', self.f1)\n",
    "        neptune.log_metric('macrof1', self.macrof1)\n",
    "#         neptune.log_metric('rmse', self.rms)\n",
    "        self.total_loss=0\n",
    "        self.batch_count=0\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         ids = batch['ids']\n",
    "#         mask = batch['mask']\n",
    "#         token_type_ids = batch['token_type_ids']\n",
    "#         targets = batch['targets']\n",
    "#         targets = targets.to(torch.float32)\n",
    "#         lx = batch['lexical']\n",
    "#         hurt = batch['hurtlex']\n",
    "#         outputs = self.forward(ids, mask, token_type_ids, lx, hurt)\n",
    "# #         outputs = self.forward(ids, mask, token_type_ids)\n",
    "#         loss = torch.nn.CrossEntropyLoss()(outputs.view(-1), targets.view(-1))\n",
    "# #         labels_hat = torch.argmax(outputs, dim=1)\n",
    "#         self.preds.extend(outputs.cpu().detach().numpy().tolist())\n",
    "#         self.targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "#         self.val_loss += loss\n",
    "#         self.val_batch += 1\n",
    "# #         val_acc = torch.sum(targets == labels_hat).item() / (len(targets) * 1.0)\n",
    "#         output = {\n",
    "#             'val_loss': loss,\n",
    "# #             'val_acc': torch.tensor(val_acc), # everything must be a tensor\n",
    "#         }\n",
    "#         return output\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        targets = batch['targets']\n",
    "        lx = batch['lexical']\n",
    "        hurt = batch['hurtlex']\n",
    "        lx = lx.to(torch.float32)\n",
    "        hurt = hurt.to(torch.float32)\n",
    "        outputs = self.forward(ids, mask, token_type_ids, lx, hurt)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "        labels_hat = torch.argmax(outputs, dim=1)\n",
    "        self.preds.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "        self.targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        self.val_loss += loss\n",
    "        self.val_batch += 1\n",
    "        val_acc = torch.sum(targets == labels_hat).item() / (len(targets) * 1.0)\n",
    "        output = {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': torch.tensor(val_acc), # everything must be a tensor\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self.preds = list(np.argmax(np.array(self.preds), axis=1).flatten())\n",
    "        print(classification_report(self.targets, self.preds, digits=4))\n",
    "        self.f1 = f1_score(self.targets, self.preds)\n",
    "        self.macrof1 = f1_score(self.targets, self.preds, average='macro')\n",
    "        self.acc = accuracy_score(self.targets, self.preds)\n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        outputs = self.forward(ids, mask, token_type_ids)\n",
    "        labels_hat = torch.argmax(outputs, dim=1)\n",
    "        self.test_preds.extend(labels_hat.cpu().detach().numpy().tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath='/scratch/tr/haha_cls1',\n",
    "    filename=params['file']+'-{epoch}',\n",
    "    save_top_k = -1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: None, using: 0 TPU cores\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    }
   ],
   "source": [
    "model = LMModelClassifier(params)\n",
    "trainer = pl.Trainer(max_epochs=params['epochs'], callbacks=[checkpoint_callback], gpus=[params['gpu']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Set SLURM handle signals.\n",
      "\n",
      "  | Name             | Type       | Params\n",
      "------------------------------------------------\n",
      "0 | l1               | XLNetModel | 116 M \n",
      "1 | pre_classifier_1 | Linear     | 590 K \n",
      "2 | hidden_lex       | Linear     | 272   \n",
      "3 | hidden_hurt      | Linear     | 306   \n",
      "4 | pre_classifier_2 | Linear     | 642 K \n",
      "5 | classifier       | Linear     | 1.6 K \n",
      "6 | dropout          | Dropout    | 0     \n",
      "------------------------------------------------\n",
      "117 M     Trainable params\n",
      "0         Non-trainable params\n",
      "117 M     Total params\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, val dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.3913    0.3462    0.3673        26\n",
      "           1     0.5854    0.6316    0.6076        38\n",
      "\n",
      "    accuracy                         0.5156        64\n",
      "   macro avg     0.4883    0.4889    0.4875        64\n",
      "weighted avg     0.5065    0.5156    0.5100        64\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The dataloader, train dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 40 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687a721dfad04900aa59c814c1972ce3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8604    0.9263    0.8921       692\n",
      "           1     0.9517    0.9061    0.9283      1108\n",
      "\n",
      "    accuracy                         0.9139      1800\n",
      "   macro avg     0.9060    0.9162    0.9102      1800\n",
      "weighted avg     0.9166    0.9139    0.9144      1800\n",
      "\n",
      "Epoch: 1, Loss:  0.3676433861255646\n",
      "https://ui.neptune.ai/tathagataraha/haha-cls1/e/HAH2-421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2136: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {progress_bar:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: The {log:dict keyword} was deprecated in 0.9.1 and will be removed in 1.0.0\n",
      "Please use self.log(...) inside the lightningModule instead.\n",
      "\n",
      "# log on a step or aggregate epoch metric to the logger and/or progress bar\n",
      "# (inside LightningModule)\n",
      "self.log('train_loss', loss, on_step=True, on_epoch=True, prog_bar=True)\n",
      "  warnings.warn(*args, **kwargs)\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/distributed.py:49: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  warnings.warn(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.fit(model, train_dataloader=training_loader, val_dataloaders=testing_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trainer, model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [00:03,  8.37it/s]\n",
      "57it [00:06,  8.37it/s]\n",
      "32it [00:03,  8.48it/s]\n",
      "57it [00:06,  8.38it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/scratch/tr/haha_cls1/46d97s2f2a49m23loqci-epoch=2.ckpt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-e24a9c0546c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mtesting_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtesting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtest_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#     model = LMModelClassifier.load_from_checkpoint(startpath+'2c1kyhlwp739kr31cpsx-epoch=0.ckpt')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLMModelClassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_from_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstartpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'-epoch='\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.ckpt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/core/saving.py\u001b[0m in \u001b[0;36mload_from_checkpoint\u001b[0;34m(cls, checkpoint_path, map_location, hparams_file, strict, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhparams_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/pytorch_lightning/utilities/cloud_io.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path_or_url, map_location)\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict_from_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mfs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_filesystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/fsspec/spec.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, path, mode, block_size, cache_options, **kwargs)\u001b[0m\n\u001b[1;32m    928\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m             \u001b[0mac\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"autocommit\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_intrans\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m             f = self._open(\n\u001b[0m\u001b[1;32m    931\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, path, mode, block_size, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_mkdir\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexist_ok\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mLocalFileOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtouch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, mode, autocommit, fs, **kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautocommit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocksize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDEFAULT_BUFFER_SIZE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/fsspec/implementations/local.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautocommit\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m\"w\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m                 \u001b[0;31m# TODO: check if path is writable?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/scratch/tr/haha_cls1/46d97s2f2a49m23loqci-epoch=2.ckpt'"
     ]
    }
   ],
   "source": [
    "test = pd.read_csv('data/public_test.csv')\n",
    "\n",
    "trainvec = pd.read_csv('hurtlex_features/public_testvec.csv')\n",
    "trainvec = trainvec.values.tolist()\n",
    "npt = 'haha-cls1'\n",
    "startpath = '/scratch/tr/haha_cls1/'\n",
    "savepath = 'test_preds/task_1/'\n",
    "with open('lexical_features/public_test.pickle', 'rb') as f:\n",
    "    lx = pickle.load(f)\n",
    "for i in range(len(trainvec)):\n",
    "    trainvec[i][1] = int(trainvec[i][1][1])\n",
    "    trainvec[i][-1] = int(trainvec[i][-1][1])\n",
    "    trainvec[i] = trainvec[i][1:]\n",
    "test['hurtlex'] = trainvec\n",
    "test['lexical'] = lx\n",
    "test_data = test.reset_index(drop=True)\n",
    "for i in range(params['epochs']):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(params['model'])\n",
    "    testing = MultiLabelDataset(test_data, tokenizer, params['max_len'], t=True)\n",
    "    test_params = {'batch_size': params['valid_batch'],\n",
    "                'shuffle': False,\n",
    "                'num_workers': 0\n",
    "                }\n",
    "    testing_loader = DataLoader(testing, **test_params)\n",
    "#     model = LMModelClassifier.load_from_checkpoint(startpath+'2c1kyhlwp739kr31cpsx-epoch=0.ckpt')\n",
    "    model = LMModelClassifier.load_from_checkpoint(startpath+params['file']+'-epoch='+str(i)+'.ckpt')\n",
    "    device = torch.device(\"cuda:\"+str(params['gpu']))\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    test_conf = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            lx = data['lexical'].to(device, dtype = torch.float)\n",
    "            hurt = data['hurtlex'].to(device, dtype = torch.float)\n",
    "            outputs = model.forward(ids, mask, token_type_ids, lx, hurt)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "#             print(outputs)\n",
    "            labels_hat = np.argmax(outputs, axis=1)\n",
    "            conf = np.max(outputs, axis=1)\n",
    "            test_preds.extend(labels_hat.tolist())\n",
    "            test_conf.extend(list(conf))\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = list(range(9001,10001))\n",
    "    df[params['label']] = list(test_preds)\n",
    "    df['conf'] = list(test_conf)\n",
    "    df.to_csv(savepath+params['file']+'-epoch='+str(i)+'.csv', index=False)\n",
    "#     train=train.reset_index(drop=True)\n",
    "    training = MultiLabelDataset(valid_data, tokenizer, params['max_len'])\n",
    "    testing_loader = DataLoader(training, **test_params)\n",
    "    test_preds = []\n",
    "    test_conf = []\n",
    "    savepath = 'train_preds/task_1/'\n",
    "    with torch.no_grad():\n",
    "        for _, data in tqdm(enumerate(testing_loader, 0)):\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n",
    "            lx = data['lexical'].to(device, dtype = torch.float)\n",
    "            hurt = data['hurtlex'].to(device, dtype = torch.float)\n",
    "            outputs = model.forward(ids, mask, token_type_ids, lx, hurt)\n",
    "            outputs = outputs.cpu().numpy()\n",
    "#             print(outputs)\n",
    "            labels_hat = np.argmax(outputs, axis=1)\n",
    "            conf = np.max(outputs, axis=1)\n",
    "            test_preds.extend(labels_hat.tolist())\n",
    "            test_conf.extend(list(conf))\n",
    "    df = pd.DataFrame()\n",
    "    df['id'] = list(range(1,1801))\n",
    "    df[params['label']] = list(test_preds)\n",
    "    df['conf'] = list(test_conf)\n",
    "    df.to_csv(savepath+params['file']+'-epoch='+str(i)+'.csv', index=False)\n",
    "#     print(len(test_preds),'dsf' ,test_preds[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
