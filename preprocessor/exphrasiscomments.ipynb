{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/dataset_creation/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "cOqUHRECEX9o",
    "outputId": "4ec4770f-ea54-4ee0-e3a4-2661629c52ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading twitter - 1grams ...\n",
      "Reading twitter - 2grams ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "# !pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cvfnKs-dEZwN",
    "outputId": "06a34c8d-e6e1-4842-8f9f-dca10138b77c"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "# !pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Wk2BUZTuEdN1",
    "outputId": "86fc9ba1-de39-4992-a083-9a760455c35a"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "# !pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqibDmdYEjIK"
   },
   "source": [
    "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SmRsC-wmEhdx",
    "outputId": "dc25dd60-4bb2-40a9-a8a4-6d367cf96c85"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "  if proc_obj == None:\n",
    "    return []\n",
    "  \n",
    "  store = []\n",
    "  for unit in proc_obj:\n",
    "    store.append(unit.match)\n",
    "  \n",
    "  return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "PI524oqVE026"
   },
   "outputs": [],
   "source": [
    "# For 2020 Datasets\n",
    "\n",
    "is_hindi = 0\n",
    "\n",
    "# For Train Data\n",
    "# datatype = \"train\"\n",
    "# For English\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\"\n",
    "\n",
    "# For Test Data\n",
    "datatype = \"train\"\n",
    "# For English\n",
    "file_name = \"data/complete_dataset.csv\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/hindi_test_1509.csv\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/german_test_1509.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "datapoints_count = 0\n",
    "see_index = True\n",
    "\n",
    "tweets = []\n",
    "raw_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "urls = []\n",
    "mentions = []\n",
    "numbers = []\n",
    "reserveds = []\n",
    "user_ID = []\n",
    "parent_ID = []\n",
    "root_ID = []\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "  stripped = []\n",
    "  for item in listie:\n",
    "    stripped.append(item.strip())\n",
    "  return stripped\n",
    "\n",
    "def hindi_clean(line, parse_obj):\n",
    "  # beta\n",
    "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
    "  valid_stri = \"\"\n",
    "\n",
    "  for raw_token in tokens:\n",
    "    token = raw_token.strip()\n",
    "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.smileys)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.emojis)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.urls)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.mentions)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.numbers)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.reserved)):\n",
    "      continue\n",
    "    valid_stri = valid_stri + \" \" + token\n",
    "  return valid_stri.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "0TuJvskdE95H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 86975\n"
     ]
    }
   ],
   "source": [
    "if datatype == 'train':\n",
    "#     workbook = xlrd.open_workbook(file_name)\n",
    "#     sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "#     for row in range(sheet.nrows):\n",
    "#         line = sheet.row_values(row)\n",
    "\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \",\")\n",
    "    flag = 0\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "#         print(line)\n",
    "#         flag += 1\n",
    "#         if flag == 2:\n",
    "#             break\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[1])\n",
    "        task_1_labels.append(line[10])\n",
    "#         task_2_labels.append(line[3])\n",
    "        user_ID.append(line[2])\n",
    "        parent_ID.append(line[3])\n",
    "        root_ID.append(line[4])\n",
    "        tweets.append(line[8].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[8].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[8].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[8].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[8].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9gtRGkIkNmIv",
    "outputId": "8a0715aa-55a2-459c-ceed-94666fe0c387"
   },
   "outputs": [],
   "source": [
    "if datatype == 'test':\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \",\")\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        task_2_labels.append(line[3])\n",
    "        hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "Gd_fy5REFxUX",
    "outputId": "f70d9fb9-2246-4c8d-f59e-5574161390b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "[\"@ApurvaU21 @NIA_India @PMOIndia I believe they already know the truth about sushant but I'm wondering what is causing the delay? What's stopping CBI?  CBI Knows SSR Culprits\", '@ItsRNeha07 @privin07 @republic Courts are saying that media should not discuss SSR case. Arnab is prevented from airing views on Republic TV.', \"@ItsRNeha07 @MadhumitaroyC @republic Yes Neha..u r right..d same strategy usd in Mansukh's n @itsSSR case.. Same no.number plates used in both cases &amp;plenty of SIM cards used by Vaze too. Was it a conspiracy..SSR changd 50 SIMs on his own or he ws forcd to do so to cutoff him 4m everyone  Interrogate Vaze 4 Sushant\", '@ItsRNeha07 @republic Smile foundation Ambulance mystery is still unsolved #JusticeForSushantSinghRajput', \"@ItsRNeha07 @Moria15162914 @republic These are ambulances of the NGO with which SSR was associated. Second body of a girl was taken by that ambulance and SSR by Congress MLA's ambulance.  SSR Ke Rang Me SSRians https://t.co/3QSy8zzMc2\", \"@ItsRNeha07 @Diptish17304859 @republic Don't be shocked   Just see d ANGLE OF LADDERS CAREFULLY   Indeed there was two GREEN twins AMBULANCE https://t.co/ObQMQ6hTgs\", \"@ItsRNeha07 @republic One car was also there outside SSR'S house having a fake number plate.\", '@ItsRNeha07 @republic Sanjayüëªaut üôÑWazeüòÇ‡§®‡•áüêß‡§¨‡§æ‡§¨‡§æ&amp;40‡§ö‡•ã‡§∞‡•ã‡§Ç ‡§ï‡§æ ‡§ñ‡•Å‡§≤‡§æ‡§∏‡§æ ‡§ï‡§ø‡§Ø‡§æ‚ÄºÔ∏è üòú‡§°‡§∞ ‡§ï‡§æ ‡§Æ‡§æ‡§π‡•å‡§≤‚ò†Ô∏èüòü  üö©‡§¶‡•á‡§∂‡§π‡§ø‡§§üö© üòéAmitShahüëâüèª‡§∂‡§∞‡•ç‡§§üö©At least Punjab,Rajasthan Congress‚ùéStepDown + All‚òëÔ∏èBillsPass (UCC+Population+NRC+HinduRashtraüö©üö©)üôèüôè https://t.co/YPQfbRYv7t https://t.co/5ETBHatNAS', '@ItsRNeha07 Hum kuch sahi bole ya galat...jo Police commissioner ko zhoota sabit kar sakte he to hum kis khet ki muli..??? Hamara kam to bas voting karna he..üòü', '@ItsRNeha07 @republic üòß']\n",
      "Raw Texts:\n",
      "[\"I believe they already know the truth about sushant but I'm wondering what is causing the delay? What's stopping CBI? CBI Knows SSR Culprits\", 'Courts are saying that media should not discuss SSR case. Arnab is prevented from airing views on Republic TV.', \"Yes Neha..u r right..d same strategy usd in Mansukh's n case.. Same no.number plates used in both cases &amplenty of SIM cards used by Vaze too. Was it a conspiracy..SSR changd SIMs on his own or he ws forcd to do so to cutoff him m everyone Interrogate Vaze Sushant\", 'Smile foundation Ambulance mystery is still unsolved', \"These are ambulances of the NGO with which SSR was associated. Second body of a girl was taken by that ambulance and SSR by Congress MLA's ambulance. SSR Ke Rang Me SSRians\", \"Don't be shocked Just see d ANGLE OF LADDERS CAREFULLY Indeed there was two GREEN twins AMBULANCE\", \"One car was also there outside SSR'S house having a fake number plate.\", 'Sanjayaut Waze&amp;40 AmitShahAt least Punjab,Rajasthan CongressStepDown + AllBillsPass (UCC+Population+NRC+HinduRashtra)', 'Hum kuch sahi bole ya galat...jo Police commissioner ko zhoota sabit kar sakte he to hum kis khet ki muli..??? Hamara kam to bas voting karna he..', '']\n",
      "Hashtags:\n",
      "[[], [], [], ['#JusticeForSushantSinghRajput'], [], [], [], [], [], []]\n",
      "Smileys:\n",
      "[[], [], [';p'], [], [], [], [], [], [], []]\n",
      "Emojis:\n",
      "[[], [], [], [], [], [], [], ['üëª', 'üôÑ', 'üòÇ', 'üêß', 'üòú', '‚ò†', 'üòü', 'üö©', 'üö©', 'üòé', 'üëâ', 'üèª', 'üö©', '‚ùé', '‚òë', 'üö©', 'üö©', 'üôè', 'üôè'], ['üòü'], ['üòß']]\n",
      "Urls:\n",
      "[[], [], [], [], ['https://t.co/3QSy8zzMc2'], ['https://t.co/ObQMQ6hTgs'], [], ['https://t.co/YPQfbRYv7t', 'https://t.co/5ETBHatNAS'], [], []]\n",
      "Mentions:\n",
      "[['@ApurvaU21', '@NIA_India', '@PMOIndia'], ['@ItsRNeha07', '@privin07', '@republic'], ['@ItsRNeha07', '@MadhumitaroyC', '@republic', '@itsSSR'], ['@ItsRNeha07', '@republic'], ['@ItsRNeha07', '@Moria15162914', '@republic'], ['@ItsRNeha07', '@Diptish17304859', '@republic'], ['@ItsRNeha07', '@republic'], ['@ItsRNeha07', '@republic'], ['@ItsRNeha07'], ['@ItsRNeha07', '@republic']]\n",
      "Numbers:\n",
      "[[], [], ['50', '4', '4'], [], [], [], [], [], [], []]\n",
      "Reserved Words:\n",
      "[[], [], [], [], [], [], [], [], [], []]\n",
      "Task Labels:\n",
      "['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "display_size = 10\n",
    "start = 100\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[start: start + display_size])\n",
    "\n",
    "print(\"Raw Texts:\")\n",
    "print(raw_tweet_texts[start: start + display_size])\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[start: start + display_size])\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[start: start + display_size])\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[start: start + display_size])\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[start: start + display_size])\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[start: start + display_size])\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[start: start + display_size])\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[start: start + display_size])\n",
    "\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[start: start + display_size])\n",
    "print(task_2_labels[start: start + display_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6sOeB9UWE7p"
   },
   "source": [
    "### Example\n",
    "#### Raw Tweet Text\n",
    "'RT @jeonggukpics: Don‚Äôt disturb please, he is enjoying his snacks while making those little dance üò≠üòÇüòÇüò≠üíú  #BBMAsTopSocial BTS #JUNGKOOK #Ï†ïÍµ≠‚Ä¶'\n",
    "#### Clean Text\n",
    "': Dont disturb please, he is enjoying his snacks while making those little dance BTS'\n",
    "\n",
    "#### Emojis\n",
    "'['üò≠', 'üòÇ', 'üòÇ', 'üò≠', 'üíú']'\n",
    "\n",
    "#### Hashtags\n",
    "''#BBMAsTopSocial', '#JUNGKOOK', '#Ï†ïÍµ≠''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "LUrX_FPcIVQR",
    "outputId": "74d61f00-c87e-48d7-98a2-2ee5d8dc3633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[[], [], [], [], [], [], [], ['ghost', 'face with rolling eyes', 'face with tears of joy', 'penguin', 'face with stuck-out tongue & winking eye', 'skull and crossbones', 'worried face', 'triangular flag', 'triangular flag', 'smiling face with sunglasses', 'backhand index pointing right', 'light skin tone', 'triangular flag', 'cross mark button', 'ballot box with check', 'triangular flag', 'triangular flag', 'folded hands', 'folded hands'], ['worried face'], ['anguished face'], [], [], ['hundred points', 'hundred points'], [], ['folded hands', 'folded hands'], [], [], [], [], [], [], [], [], [], [], [], [], ['eyes', 'eyes', 'eyes', 'eyes', 'eyes'], [], [], [], [], [], ['expressionless face'], [], [], ['smiling face with open mouth & cold sweat'], [], [], [], ['disappointed face', 'disappointed face'], ['disappointed face', 'disappointed face'], ['pensive face'], [], ['pensive face'], ['smiling face with smiling eyes', 'smiling face with smiling eyes'], ['person raising hand'], [], [], [], [], ['broken heart'], ['expressionless face', 'expressionless face', 'expressionless face'], [], [], [], [], [], [], [], [], [], [], [], ['unamused face', 'face with steam from nose', 'pensive face'], [], ['face with rolling eyes'], [], [], [], ['thumbs up', 'light skin tone', 'thumbs up', 'light skin tone'], [], [], [], ['winking face', 'OK hand', 'medium-light skin tone'], ['face screaming in fear', 'smiling face with heart-eyes'], ['OK hand', 'OK hand', 'smiling face with open mouth'], ['fire'], [], [], ['man', 'factory'], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "  texts = []\n",
    "  for emoji in emo_list:\n",
    "    text = emotext(emoji)\n",
    "    texts.append(text.replace(\"_\", \" \"))\n",
    "  emoji_texts.append(texts)\n",
    "\n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[100: 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "9PWvCf65IXQT",
    "outputId": "93f0c3df-ccb5-415e-c26b-a743d2157c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags:\n",
      "[[], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "  segmented_set = []\n",
    "  for tag in hashset:\n",
    "    word = tag[1: ]\n",
    "    # removing the hash symbol\n",
    "    segmented_set.append(seg_tw.segment(word))\n",
    "  segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags:\")\n",
    "print(segmented_hashtags[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "name = 'data/complete.pickle'\n",
    "dickie = {}\n",
    "dickie['tweet_id'] = tweet_ids\n",
    "dickie['task_1'] = task_1_labels\n",
    "# dickie['task_2'] = task_2_labels\n",
    "# dickie['hasoc_id'] = hasoc_ID\n",
    "dickie['full_tweet'] = tweets\n",
    "dickie['tweet_raw_text'] = raw_tweet_texts\n",
    "dickie['hashtags'] = hashtags\n",
    "dickie['smiley'] = smileys\n",
    "dickie['emoji'] = emojis\n",
    "dickie['url'] = urls\n",
    "dickie['mentions'] = mentions\n",
    "dickie['numerals'] = numbers\n",
    "dickie['reserved_word'] = reserveds\n",
    "dickie['emotext'] = emoji_texts\n",
    "dickie['segmented_hash'] = segmented_hashtags\n",
    "dickie['user_ID'] = user_ID\n",
    "dickie['root_ID'] = root_ID\n",
    "dickie['parent_ID'] = parent_ID\n",
    "# dickie['user_ID'] = user_ID\n",
    "with open(name, 'wb') as f:\n",
    "  pickle.dump(dickie, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FK4uO1J1LeIC",
    "outputId": "ec2a5973-5576-43de-93ec-db36098869df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975, 86975]\n"
     ]
    }
   ],
   "source": [
    "with open(name, 'rb') as f:\n",
    "  try_dict = pickle.load(f)\n",
    "\n",
    "sizes = []\n",
    "for key in try_dict.keys():\n",
    "  sizes.append(len(try_dict[key]))\n",
    "\n",
    "# Verifying if all sizes are equal\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN56TeSpn1nZ9OhsBKdO9oL",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
