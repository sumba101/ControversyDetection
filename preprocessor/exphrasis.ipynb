{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/sayarghoshroy/Hate-Speech-Detection/blob/master/dataset_creation/tweet_processor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "5SzZYXq3ER6b"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xlrd\n",
    "import re\n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "cOqUHRECEX9o",
    "outputId": "4ec4770f-ea54-4ee0-e3a4-2661629c52ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word statistics files not found!\n",
      "Downloading... done!\n",
      "Unpacking... done!\n",
      "Reading twitter - 1grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /home/tathagata.raha/.ekphrasis/stats/twitter/counts_1grams.txt\n",
      "Reading twitter - 2grams ...\n",
      "generating cache file for faster loading...\n",
      "reading ngrams /home/tathagata.raha/.ekphrasis/stats/twitter/counts_2grams.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/ekphrasis/classes/exmanager.py:14: FutureWarning: Possible nested set at position 42\n",
      "  regexes = {k.lower(): re.compile(self.expressions[k]) for k, v in\n"
     ]
    }
   ],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "# !pip install ekphrasis\n",
    "from ekphrasis.classes.segmenter import Segmenter\n",
    "# to leverage word statistics from Twitter\n",
    "seg_tw = Segmenter(corpus = \"twitter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "cvfnKs-dEZwN",
    "outputId": "06a34c8d-e6e1-4842-8f9f-dca10138b77c"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "# !pip install tweet-preprocessor\n",
    "import preprocessor as tweet_proc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "Wk2BUZTuEdN1",
    "outputId": "86fc9ba1-de39-4992-a083-9a760455c35a"
   },
   "outputs": [],
   "source": [
    "# Uncomment if you're running it for the first time\n",
    "# !pip install emot\n",
    "from emot.emo_unicode import UNICODE_EMO, EMOTICONS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HqibDmdYEjIK"
   },
   "source": [
    "#### *Raw Datasets are hosted [here](https://drive.google.com/drive/folders/1TuHRQQ41lK9oXJhlhsRiMjMnczuKn_kF?usp=sharing).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "SmRsC-wmEhdx",
    "outputId": "dc25dd60-4bb2-40a9-a8a4-6d367cf96c85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive', force_remount = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "fD2Ksnv6EmaS"
   },
   "outputs": [],
   "source": [
    "def make_list(proc_obj):\n",
    "  if proc_obj == None:\n",
    "    return []\n",
    "  \n",
    "  store = []\n",
    "  for unit in proc_obj:\n",
    "    store.append(unit.match)\n",
    "  \n",
    "  return store\n",
    "\n",
    "def emotext(text):\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\", \"\").replace(\":\", \"\").split()))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "PI524oqVE026"
   },
   "outputs": [],
   "source": [
    "# For 2020 Datasets\n",
    "\n",
    "is_hindi = 0\n",
    "\n",
    "# For Train Data\n",
    "# datatype = \"train\"\n",
    "# For English\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/english.xlsx\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/hindi.xlsx\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_data/german.xlsx\"\n",
    "\n",
    "# For Test Data\n",
    "datatype = \"train\"\n",
    "# For English\n",
    "file_name = \"data/dataset.csv\"\n",
    "\n",
    "# For Hindi\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/hindi_test_1509.csv\"\n",
    "# is_hindi = 1\n",
    "\n",
    "# For German\n",
    "# file_name = \"/content/drive/My Drive/HASOC_raw_data/2020_test_data/german_test_1509.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "nNNoGyv8E5th"
   },
   "outputs": [],
   "source": [
    "# Initializing Lists\n",
    "datapoints_count = 0\n",
    "see_index = True\n",
    "\n",
    "tweets = []\n",
    "raw_tweet_texts = []\n",
    "tokenized_tweets = []\n",
    "hashtags = []\n",
    "smileys = []\n",
    "emojis = []\n",
    "urls = []\n",
    "mentions = []\n",
    "numbers = []\n",
    "reserveds = []\n",
    "user_ID = []\n",
    "task_1_labels = []\n",
    "task_2_labels = []\n",
    "tweet_ids = []\n",
    "hasoc_ID = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "nsKZHtQ-FvOw"
   },
   "outputs": [],
   "source": [
    "def strip_list(listie):\n",
    "  stripped = []\n",
    "  for item in listie:\n",
    "    stripped.append(item.strip())\n",
    "  return stripped\n",
    "\n",
    "def hindi_clean(line, parse_obj):\n",
    "  # beta\n",
    "  tokens = line.replace(\":\", \" : \").replace(\",\", \" , \").replace(\";\", \" ; \").split(\" \")\n",
    "  valid_stri = \"\"\n",
    "\n",
    "  for raw_token in tokens:\n",
    "    token = raw_token.strip()\n",
    "    if token in strip_list(make_list(parse_obj.hashtags)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.smileys)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.emojis)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.urls)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.mentions)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.numbers)):\n",
    "      continue\n",
    "    if token in strip_list(make_list(parse_obj.reserved)):\n",
    "      continue\n",
    "    valid_stri = valid_stri + \" \" + token\n",
    "  return valid_stri.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "0TuJvskdE95H"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Datapoints: 2709\n"
     ]
    }
   ],
   "source": [
    "if datatype == 'train':\n",
    "#     workbook = xlrd.open_workbook(file_name)\n",
    "#     sheet = workbook.sheet_by_index(0)\n",
    "\n",
    "#     for row in range(sheet.nrows):\n",
    "#         line = sheet.row_values(row)\n",
    "\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \",\")\n",
    "    flag = 0\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "#         print(line)\n",
    "#         flag += 1\n",
    "#         if flag == 2:\n",
    "#             break\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[1])\n",
    "        task_1_labels.append(line[7])\n",
    "#         task_2_labels.append(line[3])\n",
    "        user_ID.append(line[3])\n",
    "        tweets.append(line[2].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[2].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[2].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[2].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[2].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "9gtRGkIkNmIv",
    "outputId": "8a0715aa-55a2-459c-ceed-94666fe0c387"
   },
   "outputs": [],
   "source": [
    "if datatype == 'test':\n",
    "    file = open(file_name, 'r')\n",
    "    file_reader = csv.reader(file, delimiter = \",\")\n",
    "    for line in file_reader:\n",
    "        if see_index == True:\n",
    "            see_index = False\n",
    "            continue\n",
    "\n",
    "        datapoints_count += 1\n",
    "        tweet_ids.append(line[0])\n",
    "        task_1_labels.append(line[2])\n",
    "        task_2_labels.append(line[3])\n",
    "        hasoc_ID.append(line[4])\n",
    "        tweets.append(line[1].replace(\"\\n\", \" \"))\n",
    "\n",
    "        parse_obj = tweet_proc.parse(line[1].replace(\"\\n\", \" \"))\n",
    "        tokenized_tweets.append(tweet_proc.tokenize(line[1].replace(\"\\n\", \" \")))\n",
    "        hashtags.append(strip_list(make_list(parse_obj.hashtags)))\n",
    "        smileys.append(strip_list(make_list(parse_obj.smileys)))\n",
    "        emojis.append(strip_list(make_list(parse_obj.emojis)))\n",
    "        urls.append(strip_list(make_list(parse_obj.urls)))\n",
    "        mentions.append(strip_list(make_list(parse_obj.mentions)))\n",
    "        numbers.append(strip_list(make_list(parse_obj.numbers)))\n",
    "        reserveds.append(strip_list(make_list(parse_obj.reserved)))\n",
    "\n",
    "        if is_hindi == 0:\n",
    "          raw_tweet_texts.append(tweet_proc.clean(line[1].replace(\"\\n\", \" \")))\n",
    "        else:\n",
    "          raw_tweet_texts.append(hindi_clean(line[1].replace(\"\\n\", \" \"), parse_obj))\n",
    "\n",
    "    print(\"Number of Datapoints: \" + str(datapoints_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 409
    },
    "id": "Gd_fy5REFxUX",
    "outputId": "f70d9fb9-2246-4c8d-f59e-5574161390b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweets:\n",
      "['\"It is impossible to overstate the dangerous precedent Mr. Assange’s indictment sets\" | Opinion | @nytimes #Assange  https://t.co/yDaHd7rjkv', \"'Assange’s liberty represents that of all journalists and publishers whose job is to expose government and corporate criminality without fear of prosecution'  https://t.co/xVlFx2FMbB\", \"UK judge to rule on US extradition for WikiLeaks' Assange | AP https://t.co/iEb0L9e2Fx\", '\"It is dangerous that they are trying to pick him off - and lock him up for a long time - on a story that leaps over and public interest hurdle\" - Former @Guardian editor-in-chief Alan Rusbridger @arusbridger #FreeAssangeNOW https://t.co/nVBlva3Xza', 'The Guardian view on Julian Assange: Do not extradite him | Editorial  The US should never have brought the case against the WikiLeaks founder. This attack on press freedom must be rejected #Assange  https://t.co/DJV1PZQcGg', 'WikiLeaks accepts submissions of diplomatic, political or historical importance which are censored or otherwise suppressed   Submit to @wikileaks: https://t.co/cLRcuIArP7  Donate: https://t.co/vvbZBOydoj', 'Watch: The Persecution of Julian Assange - Excellent summary with Glenn Greenwald | @Sulliview @suigenerisjen  https://t.co/xtQx5FtM3K', 'All major media and human rights organizations oppose the extradition proceedings against Julian Assange  Please donate to his defense here: https://t.co/hzWWnt9Nst https://t.co/YhVhtKTVla', \"Gospel Choir sing (Something Inside) So Strong' for Julian Assange/ with thanks to @PixelHELPER  #FreeAssangeNOW https://t.co/g7Cn1AKYaf\", 'The precedent being set by the prosecution of Julian Assange fundamentally undermines any Free Press protections  Turkish journalist @candundaradasi sentenced in absentia to more than 27 years in jail   https://t.co/CXWwpnzi23']\n",
      "Raw Texts:\n",
      "['\"It is impossible to overstate the dangerous precedent Mr. Assanges indictment sets\" | Opinion |', \"'Assanges liberty represents that of all journalists and publishers whose job is to expose government and corporate criminality without fear of prosecution'\", \"UK judge to rule on US extradition for WikiLeaks' Assange | AP\", '\"It is dangerous that they are trying to pick him off - and lock him up for a long time - on a story that leaps over and public interest hurdle\" - Former editor-in-chief Alan Rusbridger', 'The Guardian view on Julian Assange: Do not extradite him | Editorial The US should never have brought the case against the WikiLeaks founder. This attack on press freedom must be rejected', 'WikiLeaks accepts submissions of diplomatic, political or historical importance which are censored or otherwise suppressed Submit to : Donate:', 'Watch: The Persecution of Julian Assange - Excellent summary with Glenn Greenwald |', 'All major media and human rights organizations oppose the extradition proceedings against Julian Assange Please donate to his defense here:', \"Gospel Choir sing (Something Inside) So Strong' for Julian Assange/ with thanks to\", 'The precedent being set by the prosecution of Julian Assange fundamentally undermines any Free Press protections Turkish journalist sentenced in absentia to more than years in jail']\n",
      "Hashtags:\n",
      "[['#Assange'], [], [], ['#FreeAssangeNOW'], ['#Assange'], [], [], [], ['#FreeAssangeNOW'], []]\n",
      "Smileys:\n",
      "[[], [], [], [], [], [], [], [], [], []]\n",
      "Emojis:\n",
      "[[], [], [], [], [], [], [], [], [], []]\n",
      "Urls:\n",
      "[['https://t.co/yDaHd7rjkv'], ['https://t.co/xVlFx2FMbB'], ['https://t.co/iEb0L9e2Fx'], ['https://t.co/nVBlva3Xza'], ['https://t.co/DJV1PZQcGg'], ['https://t.co/cLRcuIArP7', 'https://t.co/vvbZBOydoj'], ['https://t.co/xtQx5FtM3K'], ['https://t.co/hzWWnt9Nst', 'https://t.co/YhVhtKTVla'], ['https://t.co/g7Cn1AKYaf'], ['https://t.co/CXWwpnzi23']]\n",
      "Mentions:\n",
      "[['@nytimes'], [], [], ['@Guardian', '@arusbridger'], [], ['@wikileaks'], ['@Sulliview', '@suigenerisjen'], [], ['@PixelHELPER'], ['@candundaradasi']]\n",
      "Numbers:\n",
      "[[], [], [], [], [], [], [], [], [], ['27']]\n",
      "Reserved Words:\n",
      "[[], [], [], [], [], [], [], [], [], []]\n",
      "Task Labels:\n",
      "['1', '1', '1', '1', '1', '1', '1', '1', '1', '1']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# Viewing Created Dataset\n",
    "display_size = 10\n",
    "start = 100\n",
    "\n",
    "print(\"Tweets:\")\n",
    "print(tweets[start: start + display_size])\n",
    "\n",
    "print(\"Raw Texts:\")\n",
    "print(raw_tweet_texts[start: start + display_size])\n",
    "\n",
    "print(\"Hashtags:\")\n",
    "print(hashtags[start: start + display_size])\n",
    "\n",
    "print(\"Smileys:\")\n",
    "print(smileys[start: start + display_size])\n",
    "\n",
    "print(\"Emojis:\")\n",
    "print(emojis[start: start + display_size])\n",
    "\n",
    "print(\"Urls:\")\n",
    "print(urls[start: start + display_size])\n",
    "\n",
    "print(\"Mentions:\")\n",
    "print(mentions[start: start + display_size])\n",
    "\n",
    "print(\"Numbers:\")\n",
    "print(numbers[start: start + display_size])\n",
    "\n",
    "print(\"Reserved Words:\")\n",
    "print(reserveds[start: start + display_size])\n",
    "\n",
    "print(\"Task Labels:\")\n",
    "print(task_1_labels[start: start + display_size])\n",
    "print(task_2_labels[start: start + display_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u6sOeB9UWE7p"
   },
   "source": [
    "### Example\n",
    "#### Raw Tweet Text\n",
    "'RT @jeonggukpics: Don’t disturb please, he is enjoying his snacks while making those little dance 😭😂😂😭💜  #BBMAsTopSocial BTS #JUNGKOOK #정국…'\n",
    "#### Clean Text\n",
    "': Dont disturb please, he is enjoying his snacks while making those little dance BTS'\n",
    "\n",
    "#### Emojis\n",
    "'['😭', '😂', '😂', '😭', '💜']'\n",
    "\n",
    "#### Hashtags\n",
    "''#BBMAsTopSocial', '#JUNGKOOK', '#정국''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "LUrX_FPcIVQR",
    "outputId": "74d61f00-c87e-48d7-98a2-2ee5d8dc3633"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emoji Descriptions:\n",
      "[[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Generating Emoji Texts\n",
    "emoji_texts = []\n",
    "\n",
    "for emo_list in emojis:\n",
    "  texts = []\n",
    "  for emoji in emo_list:\n",
    "    text = emotext(emoji)\n",
    "    texts.append(text.replace(\"_\", \" \"))\n",
    "  emoji_texts.append(texts)\n",
    "\n",
    "print(\"Emoji Descriptions:\")\n",
    "print(emoji_texts[100: 200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "9PWvCf65IXQT",
    "outputId": "93f0c3df-ccb5-415e-c26b-a743d2157c80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Hashtags:\n",
      "[['free assange now'], [], ['free assange now'], [], []]\n"
     ]
    }
   ],
   "source": [
    "# Segmenting Hashtags\n",
    "segmented_hashtags = []\n",
    "\n",
    "for hashset in hashtags:\n",
    "  segmented_set = []\n",
    "  for tag in hashset:\n",
    "    word = tag[1: ]\n",
    "    # removing the hash symbol\n",
    "    segmented_set.append(seg_tw.segment(word))\n",
    "  segmented_hashtags.append(segmented_set)\n",
    "\n",
    "print(\"Segmented Hashtags:\")\n",
    "print(segmented_hashtags[0: 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "UGPepLXoJsqR"
   },
   "outputs": [],
   "source": [
    "name = 'data/extracted.pickle'\n",
    "dickie = {}\n",
    "dickie['tweet_id'] = tweet_ids\n",
    "dickie['task_1'] = task_1_labels\n",
    "# dickie['task_2'] = task_2_labels\n",
    "# dickie['hasoc_id'] = hasoc_ID\n",
    "dickie['full_tweet'] = tweets\n",
    "dickie['tweet_raw_text'] = raw_tweet_texts\n",
    "dickie['hashtags'] = hashtags\n",
    "dickie['smiley'] = smileys\n",
    "dickie['emoji'] = emojis\n",
    "dickie['url'] = urls\n",
    "dickie['mentions'] = mentions\n",
    "dickie['numerals'] = numbers\n",
    "dickie['reserved_word'] = reserveds\n",
    "dickie['emotext'] = emoji_texts\n",
    "dickie['segmented_hash'] = segmented_hashtags\n",
    "dickie['user_ID'] = user_ID\n",
    "with open(name, 'wb') as f:\n",
    "  pickle.dump(dickie, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "FK4uO1J1LeIC",
    "outputId": "ec2a5973-5576-43de-93ec-db36098869df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709, 2709]\n"
     ]
    }
   ],
   "source": [
    "with open(name, 'rb') as f:\n",
    "  try_dict = pickle.load(f)\n",
    "\n",
    "sizes = []\n",
    "for key in try_dict.keys():\n",
    "  sizes.append(len(try_dict[key]))\n",
    "\n",
    "# Verifying if all sizes are equal\n",
    "print(sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "3FUMYDP9IaXe"
   },
   "outputs": [],
   "source": [
    "# ^_^ Thank You"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyN56TeSpn1nZ9OhsBKdO9oL",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "tweet_processor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
