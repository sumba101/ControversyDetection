{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00000-f910fae0-ba10-4708-9049-670a93f637a9",
    "tags": []
   },
   "source": [
    "- Database: Covid-fake\n",
    "- Function: cleaning\n",
    "- Desp: NA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00001-b7880f68-fb43-45dd-9963-39c76f34c960"
   },
   "source": [
    "# Necessary Imports..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cell_id": "00002-9b8431e6-7762-485a-9cad-b826b8b2e453",
    "execution_millis": 1610,
    "execution_start": 1602317375718,
    "output_cleared": false,
    "source_hash": "6a0e8856"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Project(tathagataraha/contro-base)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start writing code here...\n",
    "import pandas as pd\n",
    "import xgboost\n",
    "import re\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import neptune\n",
    "from neptunecontrib.monitoring.xgboost import neptune_callback\n",
    "with open(\"token\", \"r\") as f:\n",
    "    token = f.read()\n",
    "neptune.init(project_qualified_name='tathagataraha/contro-base',\n",
    "             api_token=token,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'rnd' : 42,\n",
    "    'tsize' : 0.20,\n",
    "    'vsize' : 0.125,\n",
    "    'label' : 'is_cont'\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00003-7462cdd6-7996-40b9-b2ad-4f9db1c30a21"
   },
   "source": [
    "# Read test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00004-79ce4e19-951f-4d1d-b372-9315510c9e68",
    "execution_millis": 105,
    "execution_start": 1602317380627,
    "output_cleared": false,
    "source_hash": "9f16c2f8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trains = pd.read_csv('data/dataset.csv')\n",
    "# test = pd.read_csv('data/public_dev.csv')\n",
    "train,test = train_test_split(trains,test_size=params['tsize'],random_state=params['rnd'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2167, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "cell_id": "00005-806e1fa7-a81f-4332-9028-e656062a1e65",
    "execution_millis": 104,
    "execution_start": 1602317385054,
    "output_cleared": false,
    "source_hash": "2ecaabe6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_name</th>\n",
       "      <th>no_of_retweets</th>\n",
       "      <th>no_of_likes</th>\n",
       "      <th>is_cont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>445</th>\n",
       "      <td>2020-04-23 23:19:24</td>\n",
       "      <td>1253463512464216068</td>\n",
       "      <td>Pandemic potentially a 'death sentence' for ma...</td>\n",
       "      <td>16589206</td>\n",
       "      <td>wikileaks</td>\n",
       "      <td>215</td>\n",
       "      <td>286</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>2020-11-06 06:35:51</td>\n",
       "      <td>1324601373296676866</td>\n",
       "      <td>Nope. You’re being deliberately dishonest.\\n\\n...</td>\n",
       "      <td>23022687</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>6385</td>\n",
       "      <td>22782</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2396</th>\n",
       "      <td>2021-02-22 02:20:25</td>\n",
       "      <td>1363674979464798213</td>\n",
       "      <td>Stars showing out at the half on ESPN.. @Brook...</td>\n",
       "      <td>19923144</td>\n",
       "      <td>NBA</td>\n",
       "      <td>75</td>\n",
       "      <td>783</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1593</th>\n",
       "      <td>2021-01-11 04:43:57</td>\n",
       "      <td>1348490811248439296</td>\n",
       "      <td>The 2022 PGA Championship will not be played a...</td>\n",
       "      <td>428333</td>\n",
       "      <td>cnnbrk</td>\n",
       "      <td>1281</td>\n",
       "      <td>13972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>2021-01-12 22:22:37</td>\n",
       "      <td>1349119620939210754</td>\n",
       "      <td>That would make too much sense. https://t.co/t...</td>\n",
       "      <td>23022687</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>3919</td>\n",
       "      <td>27139</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>2021-02-19 23:00:00</td>\n",
       "      <td>1362899765940211715</td>\n",
       "      <td>▪️ Zion coming off career-high tying 36 PTS as...</td>\n",
       "      <td>19923144</td>\n",
       "      <td>NBA</td>\n",
       "      <td>50</td>\n",
       "      <td>414</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2303</th>\n",
       "      <td>2021-02-23 22:00:01</td>\n",
       "      <td>1364334222677336070</td>\n",
       "      <td>Where else can you see LUKA MAGIC at any momen...</td>\n",
       "      <td>19923144</td>\n",
       "      <td>NBA</td>\n",
       "      <td>120</td>\n",
       "      <td>1030</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1870</th>\n",
       "      <td>2020-04-20 19:35:09</td>\n",
       "      <td>1252319916566462465</td>\n",
       "      <td>Chris had the great pleasure of performing wit...</td>\n",
       "      <td>18863815</td>\n",
       "      <td>coldplay</td>\n",
       "      <td>318</td>\n",
       "      <td>2302</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1451</th>\n",
       "      <td>2021-01-20 19:47:01</td>\n",
       "      <td>1351979563023216644</td>\n",
       "      <td>President Biden and Vice President Harris are ...</td>\n",
       "      <td>428333</td>\n",
       "      <td>cnnbrk</td>\n",
       "      <td>512</td>\n",
       "      <td>7339</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2020-11-03 06:10:41</td>\n",
       "      <td>1323507874379816960</td>\n",
       "      <td>nope. https://t.co/kNzYIBYxBc</td>\n",
       "      <td>23022687</td>\n",
       "      <td>tedcruz</td>\n",
       "      <td>2148</td>\n",
       "      <td>17720</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     time                   id  \\\n",
       "445   2020-04-23 23:19:24  1253463512464216068   \n",
       "986   2020-11-06 06:35:51  1324601373296676866   \n",
       "2396  2021-02-22 02:20:25  1363674979464798213   \n",
       "1593  2021-01-11 04:43:57  1348490811248439296   \n",
       "748   2021-01-12 22:22:37  1349119620939210754   \n",
       "2553  2021-02-19 23:00:00  1362899765940211715   \n",
       "2303  2021-02-23 22:00:01  1364334222677336070   \n",
       "1870  2020-04-20 19:35:09  1252319916566462465   \n",
       "1451  2021-01-20 19:47:01  1351979563023216644   \n",
       "999   2020-11-03 06:10:41  1323507874379816960   \n",
       "\n",
       "                                                   text   user_id  user_name  \\\n",
       "445   Pandemic potentially a 'death sentence' for ma...  16589206  wikileaks   \n",
       "986   Nope. You’re being deliberately dishonest.\\n\\n...  23022687    tedcruz   \n",
       "2396  Stars showing out at the half on ESPN.. @Brook...  19923144        NBA   \n",
       "1593  The 2022 PGA Championship will not be played a...    428333     cnnbrk   \n",
       "748   That would make too much sense. https://t.co/t...  23022687    tedcruz   \n",
       "2553  ▪️ Zion coming off career-high tying 36 PTS as...  19923144        NBA   \n",
       "2303  Where else can you see LUKA MAGIC at any momen...  19923144        NBA   \n",
       "1870  Chris had the great pleasure of performing wit...  18863815   coldplay   \n",
       "1451  President Biden and Vice President Harris are ...    428333     cnnbrk   \n",
       "999                       nope. https://t.co/kNzYIBYxBc  23022687    tedcruz   \n",
       "\n",
       "      no_of_retweets  no_of_likes  is_cont  \n",
       "445              215          286        1  \n",
       "986             6385        22782        1  \n",
       "2396              75          783        0  \n",
       "1593            1281        13972        1  \n",
       "748             3919        27139        1  \n",
       "2553              50          414        0  \n",
       "2303             120         1030        0  \n",
       "1870             318         2302        0  \n",
       "1451             512         7339        1  \n",
       "999             2148        17720        1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00006-5229c43c-176e-4e28-99ae-4fa40154c8ba",
    "execution_millis": 2,
    "execution_start": 1602317390647,
    "output_cleared": false,
    "source_hash": "3300ffa3",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00007-4e0c86f9-360f-44bb-a346-33a9f60bfa34"
   },
   "source": [
    "# Label Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00008-d2221d9d-13a4-4124-bcc1-f8e296b9025b",
    "execution_millis": 29,
    "execution_start": 1602317392478,
    "output_cleared": false,
    "source_hash": "4a70d54d",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "cell_id": "00009-27915fb6-5377-4252-b54d-f5ab82228487",
    "execution_millis": 14,
    "execution_start": 1602317413798,
    "output_cleared": false,
    "source_hash": "2ecaabe6",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>is_humor</th>\n",
       "      <th>humor_rating</th>\n",
       "      <th>humor_controversy</th>\n",
       "      <th>offense_rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>1468</td>\n",
       "      <td>customer: i'd like to return this boomerang me...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5768</th>\n",
       "      <td>5769</td>\n",
       "      <td>Keep your ears ready for next week, when The A...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5714</th>\n",
       "      <td>5715</td>\n",
       "      <td>[2 am] *5 year old sneaks into my room* 5: (wh...</td>\n",
       "      <td>1</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1578</th>\n",
       "      <td>1579</td>\n",
       "      <td>Sex and food activate the same parts of the br...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6958</th>\n",
       "      <td>6959</td>\n",
       "      <td>Gay or straight, No state should legally recog...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>6862</td>\n",
       "      <td>I got into a fight with my erection this morni...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.45</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2126</th>\n",
       "      <td>2127</td>\n",
       "      <td>Mammals evolved before flowering plants.</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6202</th>\n",
       "      <td>6203</td>\n",
       "      <td>I know several jokes in sign language. I guara...</td>\n",
       "      <td>1</td>\n",
       "      <td>2.95</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1450</th>\n",
       "      <td>1451</td>\n",
       "      <td>A black guy broke into my house last night, lu...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.80</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5067</th>\n",
       "      <td>5068</td>\n",
       "      <td>\"All any one needs is someone to step in and l...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id                                               text  is_humor  \\\n",
       "1467  1468  customer: i'd like to return this boomerang me...         1   \n",
       "5768  5769  Keep your ears ready for next week, when The A...         0   \n",
       "5714  5715  [2 am] *5 year old sneaks into my room* 5: (wh...         1   \n",
       "1578  1579  Sex and food activate the same parts of the br...         0   \n",
       "6958  6959  Gay or straight, No state should legally recog...         1   \n",
       "6861  6862  I got into a fight with my erection this morni...         1   \n",
       "2126  2127           Mammals evolved before flowering plants.         0   \n",
       "6202  6203  I know several jokes in sign language. I guara...         1   \n",
       "1450  1451  A black guy broke into my house last night, lu...         1   \n",
       "5067  5068  \"All any one needs is someone to step in and l...         0   \n",
       "\n",
       "      humor_rating  humor_controversy  offense_rating  \n",
       "1467          2.30                1.0            0.00  \n",
       "5768           NaN                NaN            0.50  \n",
       "5714          3.00                0.0            0.00  \n",
       "1578           NaN                NaN            0.10  \n",
       "6958          2.25                1.0            0.30  \n",
       "6861          2.45                1.0            1.05  \n",
       "2126           NaN                NaN            0.00  \n",
       "6202          2.95                1.0            0.90  \n",
       "1450          1.80                1.0            3.40  \n",
       "5067           NaN                NaN            0.00  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00010-d9da0c39-f1ea-46d4-9715-9cea25c13fc8"
   },
   "source": [
    "# Cleaning training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "cell_id": "00011-ef6a747a-1158-4ea4-86ed-def67676f4fb",
    "execution_millis": 339,
    "execution_start": 1602317420342,
    "output_cleared": false,
    "source_hash": "a8298672",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-11-2ebb5082d0c8>:19: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  train.text = train.text.str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "train = train.reset_index(drop=True)\n",
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "STOPWORDS = []\n",
    "\n",
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "        text: a string\n",
    "        \n",
    "        return: modified initial string\n",
    "    \"\"\"\n",
    "    text = text.lower() # lowercase text\n",
    "    text = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text. substitute the matched string in REPLACE_BY_SPACE_RE with space.\n",
    "    text = BAD_SYMBOLS_RE.sub('', text) # remove symbols which are in BAD_SYMBOLS_RE from text. substitute the matched string in BAD_SYMBOLS_RE with nothing. \n",
    "#    text = re.sub(r'\\W+', '', text)\n",
    "    text = ' '.join(word for word in text.split() if word not in STOPWORDS) # remove stopwors from text\n",
    "    return text\n",
    "train.text = train.text.apply(clean_text)\n",
    "train.text = train.text.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00012-da3221db-7d57-4404-8eaf-c8f10ac4c499"
   },
   "source": [
    "### Preparing Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "cell_id": "00013-c8cdfe1d-5b09-41d3-8f6b-a7fa5f75ec3b",
    "execution_millis": 164,
    "execution_start": 1602317421233,
    "output_cleared": false,
    "source_hash": "21ef7253"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-211f84607c0a>:4: FutureWarning: The default value of regex will change from True to False in a future version.\n",
      "  test.text = test.text.str.replace('\\d+', '')\n"
     ]
    }
   ],
   "source": [
    "test = train.sample(1000)\n",
    "test = test.reset_index(drop=True)\n",
    "test.text = test.text.apply(clean_text)\n",
    "test.text = test.text.str.replace('\\d+', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "cell_id": "00014-05bed7fb-4f79-4c70-955c-89dfa07c2a32",
    "execution_millis": 6,
    "execution_start": 1602317425170,
    "output_cleared": false,
    "source_hash": "81695e2c",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       pandemic potentially a death sentence for many...\n",
       "1       nope youre being deliberately dishonestsomeone...\n",
       "2       stars showing out at the half on espn brooklyn...\n",
       "3       the pga championship will not be played at tru...\n",
       "4       that would make too much sense https tco tgjiv...\n",
       "                              ...                        \n",
       "2162    the bands a head full of dreams documentary fi...\n",
       "2163    the international atomic energy agency has str...\n",
       "2164    texas customers could experience outages in ho...\n",
       "2165    the biden administration announces it will soo...\n",
       "2166                            uh oh https tco hkyygaokn\n",
       "Name: text, Length: 2167, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00015-5dcf0766-e3b2-4a8e-8354-ce80bf561840"
   },
   "source": [
    "# Base Line Model Used\n",
    "## 1. Naive Bayes\n",
    "## 2. Linear Classifier\n",
    "## 3. Bagging\n",
    "## 4. Boosting\n",
    "## 5. SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00016-a2a07069-13ee-4126-800e-de27e63c8cec"
   },
   "source": [
    "### Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "cell_id": "00017-aaebf049-ddf3-486a-91ba-16883f95c753"
   },
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label,  feature_vector_valid, valid_y,test_data , test_label ,is_neural_net=False, xgb = False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #print(\"In Validation Data\",metrics.accuracy_score(predictions, valid_y))\n",
    "    #applying in test data\n",
    "    predictions_test = classifier.predict(test_data)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions_test = predictions_test.argmax(axis=-1)\n",
    "    print(\"f1 score: \",f1_score(test_label,predictions_test))\n",
    "        \n",
    "    return classifier, f1_score(test_label,predictions_test), metrics.accuracy_score(test_label,predictions_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00018-b67e4027-7550-4fa0-a3d7-d5074012897e"
   },
   "source": [
    "### 1.Splitting the Data into Train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "cell_id": "00019-c6184073-f974-467e-8520-bd6764ca5d22"
   },
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(train['text'], train[params['label']], test_size = params['vsize'], random_state = params['rnd'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00020-7f8298a6-60c3-41e8-a6d2-5aa6a2864a0c"
   },
   "source": [
    "### 2. Applying WordLevel tf-idf and bi-gram tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "cell_id": "00021-a0caea99-eab7-4a1a-87b9-a89680f5b1c1"
   },
   "outputs": [],
   "source": [
    "# word level tf-idf\n",
    "tfidf_vect = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', max_features=5000)\n",
    "tfidf_vect.fit(train['text'])\n",
    "xtrain_tfidf =  tfidf_vect.transform(train_x)\n",
    "xvalid_tfidf =  tfidf_vect.transform(valid_x)\n",
    "test_tfidf   =  tfidf_vect.transform(test['text'])\n",
    "\n",
    "# ngram level tf-idf (bigram in this case)\n",
    "tfidf_vect_ngram = TfidfVectorizer(analyzer='word', token_pattern=r'\\w{1,}', ngram_range=(2,3), max_features=5000)\n",
    "tfidf_vect_ngram.fit(train['text'])\n",
    "xtrain_tfidf_ngram =  tfidf_vect_ngram.transform(train_x)\n",
    "xvalid_tfidf_ngram =  tfidf_vect_ngram.transform(valid_x)\n",
    "test_tfidf_ngram   =  tfidf_vect.transform(test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xtest_tf = tfidf_vect.transform(test.text)\n",
    "xtest_tf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('chk.pkl', 'wb+') as f:\n",
    "    pickle.dump(tfidf_vect, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00022-37905eed-3943-4f65-ad5b-1f77d5357870"
   },
   "source": [
    "#  Naive Bayes Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "cell_id": "00023-0f9c3c3a-852c-439f-a91d-2b1d2a6d376f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9851239669421487\n",
      "NB, WordLevel TF-IDF:  (MultinomialNB(), 0.9851239669421487, 0.982)\n",
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-1\n"
     ]
    }
   ],
   "source": [
    "# Naive Bayes on Word Level TF IDF Vectors\n",
    "accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y, test_tfidf, test[params['label']])\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)\n",
    "params['classifier'] = 'nb'\n",
    "params['tf'] = 'unigram'\n",
    "neptune.create_experiment(params = params)\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])\n",
    "# Naive Bayes on Ngram Level TF IDF Vectors\n",
    "# accuracy = train_model(naive_bayes.MultinomialNB(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y ,test_tfidf_ngram, test['is_humor'])\n",
    "# print (\"NB, Bi-Gram Vectors: \", accuracy)\n",
    "# params['tf'] = 'bi'\n",
    "# neptune.create_experiment(params = params)\n",
    "# neptune.log_metric('accuracy', accuracy[2])\n",
    "# neptune.log_metric('f1', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00024-254e32ab-1b1c-4686-93b0-c544911155a0"
   },
   "source": [
    "# Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "cell_id": "00025-37b71f7a-c63b-4a61-a5e4-a8f2ad512b1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9844134536505331\n",
      "LR, WordLevel TF-IDF:  (LogisticRegression(), 0.9844134536505331, 0.981)\n",
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-2\n",
      "f1 score:  0.7485677912157862\n",
      "LR, Bi-Gram Vectors:  (LogisticRegression(), 0.7485677912157862, 0.605)\n",
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-3\n"
     ]
    }
   ],
   "source": [
    "# Linear Classifier on Word Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y, test_tfidf, test[params['label']])\n",
    "print(\"LR, WordLevel TF-IDF: \", accuracy)\n",
    "params['classifier'] = 'linear'\n",
    "params['tf'] = 'unigram'\n",
    "neptune.create_experiment(params = params)\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])\n",
    "# Linear Classifier on Ngram Level TF IDF Vectors\n",
    "accuracy = train_model(linear_model.LogisticRegression(),  xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y ,test_tfidf_ngram, test[params['label']])\n",
    "print(\"LR, Bi-Gram Vectors: \", accuracy)\n",
    "params['tf'] = 'bi'\n",
    "neptune.create_experiment(params = params)\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00026-c735955f-3108-44d2-94ab-8f531fcb466e"
   },
   "source": [
    "# Bagging Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "cell_id": "00027-ee51ba23-4c96-47d8-9685-f2f501ed25af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.99009900990099\n",
      "RF, WordLevel TF-IDF:  (RandomForestClassifier(), 0.99009900990099, 0.988)\n",
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-4\n"
     ]
    }
   ],
   "source": [
    "# RF on Word Level TF IDF Vectors\n",
    "accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf, train_y, xvalid_tfidf, valid_y, test_tfidf, test[params['label']])\n",
    "print (\"RF, WordLevel TF-IDF: \", accuracy)\n",
    "params['classifier'] = 'bagging'\n",
    "params['tf'] = 'unigram'\n",
    "neptune.create_experiment(params = params)\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])\n",
    "# RF on ngram Level TF IDF Vectors\n",
    "# accuracy = train_model(ensemble.RandomForestClassifier(), xtrain_tfidf_ngram, train_y, xvalid_tfidf_ngram, valid_y ,test_tfidf_ngram, test['is_humor'])\n",
    "# print (\"RF, Bi-gram TF-IDF: \", accuracy)\n",
    "# params['tf'] = 'bi'\n",
    "# neptune.create_experiment(params = params)\n",
    "# neptune.log_metric('accuracy', accuracy[2])\n",
    "# neptune.log_metric('f1', accuracy[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = accuracy[0].predict(xtest_tf)\n",
    "preds\n",
    "df = pd.DataFrame()\n",
    "df['id'] = list(range(8001,9001))\n",
    "df['is_humor'] = list(preds)\n",
    "df.to_csv('preds1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00028-7b273e95-7405-4d2d-a01f-ab6574b3ca83"
   },
   "source": [
    "# Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "cell_id": "00029-83ccdcac-554b-431a-9ed1-cd6c9a62fc51",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-5\n",
      "[13:16:54] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { callbacks } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "f1 score:  0.9836065573770492\n",
      "Xgb, WordLevel TF-IDF:  (XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "              callbacks=[<function neptune_callback.<locals>.callback at 0x153bb705df70>],\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              gamma=0, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
      "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None), 0.9836065573770492, 0.98)\n",
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-6\n",
      "[13:16:59] WARNING: ../src/learner.cc:516: \n",
      "Parameters: { callbacks } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "f1 score:  0.7354260089686099\n",
      "Xgb, Bi-gram TF-IDF:  (XGBClassifier(base_score=0.5, booster='gbtree',\n",
      "              callbacks=[<function neptune_callback.<locals>.callback at 0x153bb9355e50>],\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              gamma=0, gpu_id=-1, importance_type='gain',\n",
      "              interaction_constraints='', learning_rate=0.300000012,\n",
      "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
      "              monotone_constraints='()', n_estimators=100, n_jobs=0,\n",
      "              num_parallel_tree=1, random_state=0, reg_alpha=0, reg_lambda=1,\n",
      "              scale_pos_weight=1, subsample=1, tree_method='exact',\n",
      "              validate_parameters=1, verbosity=None), 0.7354260089686099, 0.587)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extereme Gradient Boosting on Word Level TF IDF Vectors\n",
    "neptune.create_experiment(params = params)\n",
    "accuracy = train_model(xgboost.XGBClassifier(callbacks=[neptune_callback()]), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc(), valid_y, test_tfidf.tocsc(), test[params['label']])\n",
    "print(\"Xgb, WordLevel TF-IDF: \", accuracy)\n",
    "params['classifier'] = 'boosting'\n",
    "params['tf'] = 'unigram'\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])\n",
    "# Extereme Gradient Boosting on ngram Level TF IDF Vectors\n",
    "neptune.create_experiment(params = params)\n",
    "accuracy = train_model(xgboost.XGBClassifier(callbacks=[neptune_callback()]), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc(), valid_y, test_tfidf_ngram.tocsc(), test[params['label']])\n",
    "print(\"Xgb, Bi-gram TF-IDF: \", accuracy)\n",
    "params['tf'] = 'bi'\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "00030-bd9ebf0a-d0b4-4fb7-a645-06fa684923bf"
   },
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "cell_id": "00031-446b3c68-baec-481e-9c0d-01a5f58c7fdb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9917355371900827\n",
      "SVM, WordLevel TF-IDF:  (SVC(), 0.9917355371900827, 0.99)\n",
      "https://ui.neptune.ai/tathagataraha/contro-base/e/CON-7\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#SVM Model on Unigram TF-IDF\n",
    "accuracy = train_model(svm.SVC(), xtrain_tfidf.tocsc(), train_y, xvalid_tfidf.tocsc(), valid_y, test_tfidf.tocsc(), test[params['label']])\n",
    "print(\"SVM, WordLevel TF-IDF: \", accuracy)\n",
    "params['classifier'] = 'svm'\n",
    "params['tf'] = 'unigram'\n",
    "neptune.create_experiment(params = params)\n",
    "neptune.log_metric('accuracy', accuracy[2])\n",
    "neptune.log_metric('f1', accuracy[1])\n",
    "# with open('/scratch/tathagataraha/covid-svm-uni.pkl', 'wb+') as f:\n",
    "#     pickle.dump(model, f)\n",
    "# SVM Model on Bigram TF-IDF\n",
    "# model, accuracy = train_model(svm.SVC(), xtrain_tfidf_ngram.tocsc(), train_y, xvalid_tfidf_ngram.tocsc(), valid_y, test_tfidf_ngram.tocsc(), test['label'])\n",
    "# print(\"SVM, Bi-gram TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "00032-805d7b22-27fd-4b76-8136-88f2983811b1"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "083d58b7-ade2-4beb-9a11-d89206177ed2",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
