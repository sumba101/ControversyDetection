{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "floating-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json \n",
    "import requests\n",
    "import time\n",
    "import pandas as pd\n",
    "from collections import defaultdict as dd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fuzzy-fossil",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/final.pickle', 'rb') as f:\n",
    "    tmp = pickle.load(f)\n",
    "    df = pd.DataFrame.from_dict(tmp)\n",
    "    \n",
    "# df = pd.read_csv('data/final.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "urban-columbia",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tired-owner",
   "metadata": {},
   "outputs": [],
   "source": [
    "parents = df[df['parent_ID'] == '-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "mexican-sierra",
   "metadata": {},
   "outputs": [],
   "source": [
    "comments = df[df['parent_ID'] != '-1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "lesbian-burning",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdict = parents.reset_index(drop=True).to_dict()\n",
    "new = dd()\n",
    "values = dd()\n",
    "for i in range(len(pdict['tweet_id'])):\n",
    "    parent = pdict['tweet_id'][i]\n",
    "    tmp = comments[comments['parent_ID']==parent]\n",
    "    new[parent] = tmp.tweet_raw_text.values\n",
    "    values[parent] = pdict['task_1'][i]\n",
    "#     tmp1.append(tmp.shape[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fatty-relaxation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(337, 358)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ones = 0\n",
    "zeros = 0\n",
    "for i in values.keys():\n",
    "    if values[i] == '1':\n",
    "        ones += 1\n",
    "    else:\n",
    "        zeros += 1\n",
    "ones, zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "speaking-shell",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26144"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count = 0\n",
    "counts = []\n",
    "for i in new.keys():\n",
    "    tmp = []\n",
    "    for j in new[i]:\n",
    "        if '&gt' not in j or j.strip() != '':\n",
    "            tmp.append([count, j])\n",
    "            count += 1\n",
    "    counts.append(len(tmp))\n",
    "    new[i] = tmp\n",
    "#     print(new[i])\n",
    "#     break\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "assumed-cinema",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TOXICITY_WHOLE', 'TOXICITY_RAW', 'SEVERE_TOXICITY_WHOLE', 'SEVERE_TOXICITY_RAW', 'TOXICITY_FAST_WHOLE', 'TOXICITY_FAST_RAW', 'IDENTITY_ATTACK_WHOLE', 'IDENTITY_ATTACK_RAW', 'INSULT_WHOLE', 'INSULT_RAW', 'PROFANITY_WHOLE', 'PROFANITY_RAW', 'THREAT_WHOLE', 'THREAT_RAW', 'SEXUALLY_EXPLICIT_WHOLE', 'SEXUALLY_EXPLICIT_RAW', 'OBSCENE_WHOLE', 'OBSCENE_RAW', 'RAW_SPAN', 'WHOLE_SPAN'])\n"
     ]
    }
   ],
   "source": [
    "with open('data/perspective_scores.pickle','rb') as f:\n",
    "    pers_scores = pickle.load(f)\n",
    "    print(pers_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "hundred-voluntary",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dflt():\n",
    "    return -1\n",
    "deleted = []\n",
    "for i in pers_scores.keys():\n",
    "    if len(pers_scores[i]) == 0:\n",
    "        deleted.append(i)\n",
    "    else:\n",
    "        pers_scores[i] = dd(dflt, pers_scores[i])\n",
    "for i in deleted:\n",
    "    pers_scores.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "rental-terrorist",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['TOXICITY_WHOLE', 'SEVERE_TOXICITY_WHOLE', 'TOXICITY_FAST_WHOLE', 'IDENTITY_ATTACK_WHOLE', 'INSULT_WHOLE', 'PROFANITY_WHOLE', 'THREAT_WHOLE', 'SEXUALLY_EXPLICIT_WHOLE', 'OBSCENE_WHOLE'])\n"
     ]
    }
   ],
   "source": [
    "print(pers_scores.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "arabic-cleaner",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3372: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "/home/tathagata.raha/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/_methods.py:170: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "final_scores = {\n",
    "    'ids' : [],\n",
    "    'targets' : [],\n",
    "    'scores' : []\n",
    "}\n",
    "for i in new.keys():\n",
    "    comments_score = []\n",
    "    cmt_count = 0\n",
    "    for j in new[i]:\n",
    "        if pers_scores['TOXICITY_WHOLE'][j[0]]!= -1:\n",
    "            cmt_count += 1\n",
    "            tmp1 = []\n",
    "            for k in pers_scores.keys():\n",
    "                tmp1.append(pers_scores[k][j[0]])\n",
    "            comments_score.append(tmp1)\n",
    "    comments_score = np.array(comments_score)\n",
    "    final_scores['ids'].append(i)\n",
    "    final_scores['targets'].append(values[i])\n",
    "    final_scores['scores'].append(np.mean(comments_score, axis=0))\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "hourly-slope",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost\n",
    "import re\n",
    "from sklearn import model_selection, preprocessing, linear_model, naive_bayes, metrics, svm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn import decomposition, ensemble\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "chronic-punishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(classifier, feature_vector_train, label,  feature_vector_valid, valid_y,test_data , test_label ,is_neural_net=False, xgb = False):\n",
    "    # fit the training dataset on the classifier\n",
    "    classifier.fit(feature_vector_train, label)\n",
    "    # predict the labels on validation dataset\n",
    "    predictions = classifier.predict(feature_vector_valid)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions = predictions.argmax(axis=-1)\n",
    "    \n",
    "    #print(\"In Validation Data\",metrics.accuracy_score(predictions, valid_y))\n",
    "    #applying in test data\n",
    "    predictions_test = classifier.predict(test_data)\n",
    "    \n",
    "    if is_neural_net:\n",
    "        predictions_test = predictions_test.argmax(axis=-1)\n",
    "    print(\"f1 score: \",f1_score(test_label,predictions_test))\n",
    "        \n",
    "    return classifier, f1_score(test_label,predictions_test), metrics.accuracy_score(test_label,predictions_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "regulated-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_scores['scores'])):\n",
    "    if final_scores['targets'][i] == '1':\n",
    "        final_scores['targets'][i] = 1\n",
    "    else:\n",
    "        final_scores['targets'][i] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "lesbian-incentive",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(final_scores['scores'])):\n",
    "    if final_scores['scores'][i].shape == ():\n",
    "        del final_scores['ids'][i]\n",
    "        del final_scores['targets'][i]\n",
    "        del final_scores['scores'][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "thirty-clearance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into training and validation datasets \n",
    "from sklearn.model_selection import train_test_split\n",
    "# train_x, valid_x, train_y, valid_y = model_selection.train_test_split(train['tweet'], train['label'])\n",
    "train_x, valid_x, train_y, valid_y = train_test_split(final_scores['scores'], final_scores['targets'], test_size = 0.2, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "joint-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.7703703703703704\n",
      "NB, WordLevel TF-IDF:  (LogisticRegression(), 0.7703703703703704, 0.7769784172661871)\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(linear_model.LogisticRegression(), train_x, train_y, valid_x, valid_y, valid_x, valid_y)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "overall-guidance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.881118881118881\n",
      "NB, WordLevel TF-IDF:  (RandomForestClassifier(), 0.881118881118881, 0.8776978417266187)\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(ensemble.RandomForestClassifier(), train_x, train_y, valid_x, valid_y, valid_x, valid_y)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "central-senior",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 score:  0.9090909090909092\n",
      "NB, WordLevel TF-IDF:  (SVC(), 0.9090909090909092, 0.9064748201438849)\n"
     ]
    }
   ],
   "source": [
    "accuracy = train_model(svm.SVC(), train_x, train_y, valid_x, valid_y, valid_x, valid_y)\n",
    "print (\"NB, WordLevel TF-IDF: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "dried-woman",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "all input arrays must have the same shape",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-59-0840549ae893>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/anaconda/envs/p3/lib/python3.9/site-packages/numpy/core/shape_base.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out)\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0mshapes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'all input arrays must have the same shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m     \u001b[0mresult_ndim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: all input arrays must have the same shape"
     ]
    }
   ],
   "source": [
    "np.stack(train_x, axis = 0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "satisfied-instrumentation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "()\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n",
      "(9,)\n"
     ]
    }
   ],
   "source": [
    "for i in train_x:\n",
    "    print(i.shape)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proprietary-roots",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
