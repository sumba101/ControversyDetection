{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "based-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict as dd\n",
    "import numpy as np\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, BertConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-simon",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LMModelClassifier.load_from_checkpoint('/scratch/tr/'+params['model']+'-epoch=3.ckpt')\n",
    "tokenizer = AutoTokenizer.from_pretrained(params['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "spanish-defense",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = {\n",
    "    'root': \"Hey Everyone\",\n",
    "    'comments': [\n",
    "        'hello',\n",
    "        'nice to meet you',\n",
    "        'sup bro!'\n",
    "    ]\n",
    "}\n",
    "params = {\n",
    "    'model' : 'roberta-base',\n",
    "    'label' : 'is_cont',\n",
    "    'valid_size' : 0,\n",
    "    'rnd' : 42,\n",
    "    'max_len' : 64,\n",
    "    'train_batch' : 32,\n",
    "    'valid_batch' : 32,\n",
    "    'epochs' : 4,\n",
    "    'lr' : 1e-05,\n",
    "    'dropout' : 0.1,\n",
    "    'pers' : True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "sudden-fetish",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pers_scores(comments):\n",
    "    api_key = 'AIzaSyBLnXd0ElYhQ9WzUaN-9sI4fPavky3md3o'\n",
    "    url = ('https://commentanalyzer.googleapis.com/v1alpha1/comments:analyze' +    \n",
    "        '?key=' + api_key)\n",
    "    lg = ['en']\n",
    "    attr = ['TOXICITY', 'SEVERE_TOXICITY', 'IDENTITY_ATTACK', 'INSULT', 'PROFANITY', 'THREAT', 'SEXUALLY_EXPLICIT', 'OBSCENE']\n",
    "    attr_dict = {}\n",
    "    attr_results = {}\n",
    "    for i in attr:\n",
    "        attr_dict[i] = dd()\n",
    "        attr_results[i+'_WHOLE'] = dd()\n",
    "    sum = np.array([0,0,0,0,0,0,0,0]).astype(np.float)\n",
    "    for i in comments:\n",
    "        data_dict = {\n",
    "                'comment': {'text': i[1]},\n",
    "                'languages': lg,\n",
    "                'requestedAttributes': attr_dict\n",
    "            }\n",
    "        time.sleep(1.2)\n",
    "        response = requests.post(url=url, data=json.dumps(data_dict))\n",
    "        response_dict = json.loads(response.content)\n",
    "        score = []\n",
    "        for i in attr:\n",
    "            score.append(response_dict[\"attributeScores\"][i][\"summaryScore\"][\"value\"])\n",
    "        sum += np.array(score)\n",
    "    return sum/len(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "agreed-survivor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LMModelClassifier(pl.LightningModule):\n",
    "    def __init__(self, params):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.l1 = AutoModel.from_pretrained(params['model'])\n",
    "        self.pre_classifier_1 = torch.nn.Linear(768, 768)\n",
    "        self.final_layer_dim = 768\n",
    "        if params['pers']:\n",
    "            self.pre_classifier_2 = torch.nn.Linear(8, 8)\n",
    "            self.final_layer_dim += 8\n",
    "        self.pre_classifier_x = torch.nn.Linear(self.final_layer_dim, self.final_layer_dim)\n",
    "        self.dropout = torch.nn.Dropout(params['dropout'])\n",
    "        self.total_loss = 0\n",
    "        self.batch_count = 0\n",
    "        self.epoch = 0\n",
    "        self.classifier = torch.nn.Linear(self.final_layer_dim, 2)\n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "        self.test_preds = []\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids, pers):\n",
    "        output_1 = self.l1(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_state_1 = output_1[0]\n",
    "        pooler_1 = hidden_state_1[:, 0]\n",
    "        pooler_1 = self.pre_classifier_1(pooler_1)\n",
    "        pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "        pooler_1 = self.dropout(pooler_1)\n",
    "#         pre_final1 = self.classifier(pooler_1)\n",
    "        if params['pers']:\n",
    "            pooler_2 = self.pre_classifier_2(pers)\n",
    "            pooler_2 = torch.nn.Tanh()(pooler_2)\n",
    "            pooler_2 = self.dropout(pooler_2)\n",
    "            pooler_1 = torch.cat((pooler_1, pooler_2), 1)\n",
    "            pooler_1 = self.pre_classifier_x(pooler_1)\n",
    "            pooler_1 = torch.nn.Tanh()(pooler_1)\n",
    "            pooler_1 = self.dropout(pooler_1)\n",
    "        output = self.classifier(pooler_1)\n",
    "        return output\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(params =  self.parameters(), lr=params['lr'])\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_nb):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        pers = batch['pers_scores']\n",
    "        targets = batch['targets']\n",
    "        outputs = self.forward(ids, mask, token_type_ids, pers)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "        self.total_loss += loss\n",
    "        self.batch_count += 1\n",
    "        logger_logs = {'training_loss': loss}\n",
    "        logger_logs = {'losses': logger_logs} # optional (MUST ALL BE TENSORS)\n",
    "        output = {\n",
    "            'loss': loss, # required\n",
    "            'progress_bar': {'training_loss': loss}, # optional (MUST ALL BE TENSORS)\n",
    "            'log': logger_logs\n",
    "        }\n",
    "        # return a dict\n",
    "        return output\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        self.epoch += 1\n",
    "        print(f'Epoch: {self.epoch}, Loss:  {self.total_loss/self.batch_count}')\n",
    "        self.total_loss=0\n",
    "        self.batch_count=0\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        pers = batch['pers_scores']\n",
    "        targets = batch['targets']\n",
    "        outputs = self.forward(ids, mask, token_type_ids, pers)\n",
    "        loss = torch.nn.CrossEntropyLoss()(outputs, targets)\n",
    "        labels_hat = torch.argmax(outputs, dim=1)\n",
    "        self.preds.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "        self.targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "        val_acc = torch.sum(targets == labels_hat).item() / (len(targets) * 1.0)\n",
    "        output = {\n",
    "            'val_loss': loss,\n",
    "            'val_acc': torch.tensor(val_acc), # everything must be a tensor\n",
    "        }\n",
    "        return output\n",
    "    \n",
    "    def validation_epoch_end(self, validation_step_outputs):\n",
    "        self.preds = list(np.argmax(np.array(self.preds), axis=1).flatten())\n",
    "        print(classification_report(self.targets, self.preds, digits=4))\n",
    "        self.preds = []\n",
    "        self.targets = []\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        ids = batch['ids']\n",
    "        mask = batch['mask']\n",
    "        token_type_ids = batch['token_type_ids']\n",
    "        pers = batch['pers_scores']\n",
    "        outputs = self.forward(ids, mask, token_type_ids, pers)\n",
    "        labels_hat = torch.argmax(outputs, dim=1)\n",
    "        self.test_preds.extend(labels_hat.cpu().detach().numpy().tolist())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "certain-knife",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, params, tweet):\n",
    "    inputs = tokenizer.encode_plus(\n",
    "            tweet['root'],\n",
    "            None,\n",
    "            truncation=True,\n",
    "            add_special_tokens=True,\n",
    "            max_length=params['max_len'],\n",
    "            pad_to_max_length=True,\n",
    "            return_attention_mask = True,\n",
    "            return_token_type_ids=True,\n",
    "        )\n",
    "    ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n",
    "    mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n",
    "    token_type_ids = torch.tensor(inputs[\"token_type_ids\"], dtype=torch.long)\n",
    "    pers = torch.tensor(get_pers_scores(tweet['comments']), dtype=torch.float)\n",
    "#     return ids, mask, token_type_ids, pers\n",
    "    res = model.forward(torch.reshape(ids, (1,-1)), torch.reshape(mask, (1,-1)), torch.reshape(token_type_ids, (1,-1)), torch.reshape(pers, (1, -1)))\n",
    "    return torch.argmax(res, dim=1).cpu().detach().numpy().tolist()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "elegant-turner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04880512, 0.01899365, 0.02093932, 0.0198335 , 0.03379883,\n",
       "       0.06537544, 0.05698507, 0.04165333])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get_pers_scores(tweet['comments'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "critical-minister",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "reduced-vatican",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = predict(model, tokenizer, params, tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "thrown-stylus",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fresh-protein",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
